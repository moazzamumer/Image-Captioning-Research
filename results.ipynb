{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Description', 'Predicted Caption'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv(\"trained_blip_results.csv\")\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m smoothing_function \u001b[38;5;241m=\u001b[39m SmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# BLEU Score\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m corpus_bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_captions_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicted_captions_tokenized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoothing_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmoothing_function\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorpus BLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorpus_bleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# METEOR Score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Moazzam Umer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m references, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_references, hypotheses):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# For each order of ngram, calculate the numerator and\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# denominator for the corpus-level modified precision.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_weight_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m         p_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m         p_numerators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mnumerator\n\u001b[0;32m    212\u001b[0m         p_denominators[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_i\u001b[38;5;241m.\u001b[39mdenominator\n",
      "File \u001b[1;32mc:\\Users\\Moazzam Umer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001b[39;00m\n\u001b[0;32m    366\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28msum\u001b[39m(counts\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenominator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file = \"trained_blip_results.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Ensure the required columns exist\n",
    "if not {'Original Description', 'Predicted Caption'}.issubset(data.columns):\n",
    "    raise ValueError(\"CSV must contain 'Original Description' and 'Predicted Caption' columns.\")\n",
    "\n",
    "# Extract the original and predicted captions\n",
    "original_captions = data['Original Description'].tolist()\n",
    "predicted_captions = data['Predicted Caption'].tolist()\n",
    "\n",
    "# Preprocessing: Tokenize captions for BLEU\n",
    "original_captions_tokenized = [[caption.split()] for caption in original_captions]\n",
    "predicted_captions_tokenized = [caption.split() for caption in predicted_captions]\n",
    "\n",
    "#print(original_captions_tokenized)\n",
    "\n",
    "# Initialize the smoothing function\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "# BLEU Score\n",
    "corpus_bleu_score = corpus_bleu(\n",
    "    original_captions_tokenized, \n",
    "    predicted_captions_tokenized,\n",
    "    smoothing_function=smoothing_function\n",
    ")\n",
    "print(f\"Corpus BLEU Score: {corpus_bleu_score:.4f}\")\n",
    "\n",
    "# METEOR Score\n",
    "meteor_scores = [\n",
    "    meteor_score([orig], pred)\n",
    "    for orig, pred in zip(original_captions, predicted_captions)\n",
    "]\n",
    "avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "print(f\"Average METEOR Score: {avg_meteor_score:.4f}\")\n",
    "\n",
    "# ROUGE Scores\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(predicted_captions, original_captions, avg=True)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"  ROUGE-1: {rouge_scores['rouge-1']}\")\n",
    "print(f\"  ROUGE-2: {rouge_scores['rouge-2']}\")\n",
    "print(f\"  ROUGE-L: {rouge_scores['rouge-l']}\")\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_file = \"evaluation_scores.csv\"\n",
    "evaluation_results = {\n",
    "    \"Metric\": [\"BLEU\", \"METEOR\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
    "    \"Score\": [\n",
    "        corpus_bleu_score,\n",
    "        avg_meteor_score,\n",
    "        rouge_scores[\"rouge-1\"][\"f\"],\n",
    "        rouge_scores[\"rouge-2\"][\"f\"],\n",
    "        rouge_scores[\"rouge-l\"][\"f\"],\n",
    "    ]\n",
    "}\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
